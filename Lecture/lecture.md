# Пленоптическая камера

Ключевое понятие для пленоптической съемки — это световое поле, то есть мы в каждой точке фиксируем не цвет пикселя, а двумерную матрицу пикселей, превращая жалкий двумерный кадр в нормальный четырехмерный (обычно пока с очень небольшим разрешением по t и s):

<div align="center">
<img width=800 src=img/light-field.png/></div>

На практике, съемку четырехмерного кадра светового поля обеспечивает массив микролинз, расположенный перед сенсором камеры:

<div align="center">
<img width=800 src=img/refocusing.gif/></div>

Придумана в 1994-м, собрана в Стенфорде в 2004, первая потребительская камера — Lytro, выпущена в 2012-м. С похожими технологиями сейчас активно экспериментирует VR-индустрия. От обычной камеры пленоптическая отличается лишь одной модификацией — матрица в ней накрыта сеткой из линз, каждая из которых покрывает несколько реальных пикселей. 
Если правильно рассчитать расстояние от сетки до матрицы и размер диафрагмы, в итоговом изображении получатся четкие кластеры из пикселей — эдакие мини-версии оригинального изображения.

<div align="center">
<img width=800 src=img/pixels.jpg/></div>

Оказывается, если взять из каждого кластера, скажем, один центральный пиксель и склеить картинку только по ним — она ничем не будет отличаться от снятой на обычную камеру. Да, мы немного потеряли в разрешении, но просто попросим Sony досыпать еще мегапикселей в новых матрицах.
Веселье же на этом только начинается. Если взять другой пиксель из каждого кластера и снова склеить картинку — получится снова нормальная фотография, только как будто снятая со сдвигом на один пиксель. Таким образом, имея кластеры 10x10 пикселей, мы получим 100 изображений предмета с «немного» разных точек.

<div align="center">
<img width=800 src=img/jpeg.jpg/></div>

Больше размер кластера — больше изображений, но меньше разрешение. В мире смартфонов с мегапиксельными матрицами мы хоть и можем немного пренебречь разрешением, но у всего есть предел, приходится сохранять баланс.<br />
Окей, мы собрали пленоптическую камеру, и что это нам даёт?

## 1. Честный рефокус
Имеется возможность честной корректировки фокуса в пост-продакшене. Под честной имеется в виду, что мы не применяем всякие алгоритмы деблюринга, а используем исключительно имеющиеся под рукой пиксели, выбирая или усредняя их из кластеров в нужном порядке.

<div align="center">
<img width=800 src=img/true-refocus.jpg/></div>

RAW-фотография с пленоптической камеры выглядит странно. Чтобы получить из неё привычный резкий джипег, надо сначала его собрать. Для этого надо выбрать каждый пиксель джипега из одного из кластеров RAW'а. В зависимости от того, как мы их выберем, будет меняться результат.<br />
Например, чем дальше находится кластер от места падения оригинального луча, тем более этот луч получается в расфокусе. Потому что оптика. Чтобы получить смещённое по фокусу изображение, нам лишь надо выбрать пиксели на нужном нам удалении от оригинального — либо ближе, либо дальше.

<div align="center">
<img width=800 src=img/calculate.jpg/></div>

Картинку надо читать справа налево — мы как бы восстанавливаем изображение, зная пиксели на матрице. Сверху получаем чёткий оригинал, снизу — вычисляем то, что было за ним. То есть вычислительно сдвигаем фокус. <br >
Со сдвигом фокуса на себя было сложнее — чисто физически таких пикселей в кластерах было меньше. Сначала разработчики даже не хотели давать пользователю возможность фокусироваться руками — камера сама решала это программно. Пользователям такое будущее не понравилось, потому фичу добавили в поздних прошивках под названием «креативный режим», но сделали рефокус в нём сильно ограниченным ровно по этой причине.

## 2. Карта глубины и 3D с одной камеры
Одна из самых простых операций в пленоптике — получение карты глубины. Для этого надо просто собрать два разных кадра и расчитать насколько сдвинуты объекты на них. Больше сдвиг — дальше от камеры. <br />
Недавно Google купил и убил Lytro, но использовал их технологии для своего VR и... для камеры в Pixel. Начиная с Pixel 2 камера впервые стала «немного» пленоптической, правда с кластерами всего по два пикселя. Это дало возможность гуглу не ставить вторую камеру как все остальные ребята, а вычислять карту глубины исключительно по одной фотографии.

<div align="center">
<img width=800 src=img/depth-map.jpg/></div>

Карта глубины строится по двум кадрам, сдвинутых на один суб-пиксель. Этого вполне хватает, чтобы вычислить бинарную карту глубины и отделить передний план от заднего чтобы размыть его в модном нынче боке. Результат такого расслоения еще сглаживается и «улучшается» нейросетями, которые натренированы улучшать карты глубины (а не блюрить, как многие думают).

## 3. Нарезка на слои и объекты
Вы не видите своего носа, потому что мозг склеивает вам итоговое изображение из двух разных глаз. Закройте один и вы заметите с краю целую египетскую пирамиду. <br />
Тот же эффект достижим в пленоптической камере. Собрав сдвинутые относительно друг друга изображения из пикселей разных кластеров, мы сможем посмотреть на предмет как будто с нескольких точек. Прямо как наши глаза. Что открывает нам две крутые возможности: оценку примерного расстояния до объектов, что, как и в жизни, позволяет нам легко отделить передний план от заднего, а так же, если размеры объекта небольшие, позволяет полностью удалить его из кадра. Как нос. Оптически, по-настоящему и без фотошопа.<br />
Из приходящих на ум примеров применения: вырезать деревья между камерой и объектом или удалять падающие конфетти, как на видео ниже.

<div align="center">
<img width=800 src=img/layers.jpg/></div>

## 4. «Оптическая» стабилизация без оптики
Из пленоптического RAW'а можно собрать сотню фотографий, снятых со сдвигом в несколько пикселей по всей площади матрицы. Получается, у нас есть труба диаметром с наш объектив, в рамках которой мы можем свободно перемещать точку съемки, компенсируя тем самым тряску изображения.

<div align="center">
<img width=800 src=img/stub.jpg/></div>

Технически, стабилизация всё еще оптическая, потому что нам не надо ничего вычислять — мы просто выбираем пиксели в нужных местах. С другой стороны, любая пленоптическая камера жертвует количеством мегапикселей в угоду пленоптических возможностей, а точно так же работает любой цифровой стаб.

## 5. Борьба с фильтром Байера
Даже в пленоптической камере он всё еще необходим, ведь мы так и не придумали другого способа получить цветное цифровое изображение. Зато теперь мы можем усреднять цвет не только по группке соседних пикселей, как в классическом демозаике, а по десяткам его копий в соседних кластерах.
В статьях это называют «вычисляемым супер-разрешением», ведь по сути мы сначала уменьшаем реальное разрешение матрицы в те самые десятки раз, чтобы потом как бы гордо его восстановить.

<div align="center">
<img width=800 src=img/super-permission.jpg/></div>

## 6. Вычисляемая форма диафрагмы (боке)
Любители снимать боке-сердечки здесь будут в восторге. Раз уж мы умеем управлять рефокусом, можно пойти и дальше — брать лишь некоторые пиксели из расфокусированного изображения, а другие из обычного. Так можно получить диафрагму любой формы на радость фотопабликам.

<div align="center">
<img width=800 src=img/diaphragms.jpg/></div>

## Пленоптическая камера в смартфоне
Обычно, когда речь заходит про пленоптику сегодня, то идут типичные разговоры в стиле «Шеф всё пропало: гипс снимают, клиент уезжает!», Lytro в 2019 году задешево купил Google, эксперименты Pelican Imaging в массовое производство не пошли и вообще все осталось лишь красивой теорией… <br />
Но слухи о смерти пленоптики сильно преувеличены. Напротив, сейчас пленоптические сенсоры выпускаются и используются в масштабах, которых ранее никогда не было в истории.<br />
Небезызвестная компания Google в своих смартфонах Google Pixel использует пленоптический сенсор. Телефон делает очень даже неплохое размытие заднего плана, в общем-то не хуже, чем его «трехглазые» и «четырехглазые» коллеги (согласитесь следующее фото особенно сильно выигрывает от этого эффекта):

<div align="center">
<img width=800 src=img/pixel.png/></div>

Как они это делают? Применяется неизбывный в последнее время источник волшебных чудес — недетская магия нейросетей?
Нейросети, особенно в Pixel 5, в этой задаче тоже активно применяются. А секрет качества эффекта в том, что сенсор смартфона пленоптический, правда линза накрывает только несколько пикселей, как показано на картинке:

<div align="center">
<img width=800 src=img/sensor.png/></div>

В итоге у нас получается микро-стерео картинка, здесь по движению виден очень небольшой сдвиг (правая картинка анимирована и движется вверх-вниз, поскольку так держали телефон при съемке):

<div align="center">
<img width=800 src=img/shift.gif/></div>

Но даже этого крайне небольшого сдвига хватает для того, чтобы построить карту глубины для снимаемого фото. При этом с применением машинного обучения результаты можно заметно улучшить и это уже реализовано в Pixel. В примере ниже Learned означает Stereo+Learned:

<div align="center">
<img width=800 src=img/stereo.jpg/></div>

Хорошо видно, что не все идеально, и у нас есть типичные артефакты, характерные для построения карты глубины из стерео, да и маленький параллакс сказывается. Но уже понятно, что качество глубины получается вполне достаточное для того, чтобы уверенно сегментировать изображение по глубине и дальше накладывать разные эффекты, более уверенно добавлять объекты в сцену и так далее.

## Подводя итоги
Наиболее актуально для индустрии смартфонов возможность пленоптической камеры измерять глубину одним сенсором (потенциально — быстро) и эта возможность скоро возможно будет востребована. Несколько производителе используют эту технологию. Причем ключевым драйвером следующего этапа будет дополненная реальность на смартфонах и планшетах, которой сегодня очень не хватает точности и возможности «видеть» трехмерную сцену.
